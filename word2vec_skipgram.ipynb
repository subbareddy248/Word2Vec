{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing corpus into vocabulary\n",
    "\n",
    "def process_corp(corpus):\n",
    "    words = re.sub(\" \\d+\", \" \", corpus)\n",
    "    words = words.split()\n",
    "    unique_words = set(words)\n",
    "    vocab = list(unique_words)\n",
    "    return vocab\n",
    "\n",
    "vocab_input = process_corp(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating one hot vectors for voacbulary\n",
    "\n",
    "def one_hot_encoding(vocab_input,list_of_zeroes):\n",
    "    encoded_vocab = []\n",
    "    for i in range(len(vocab_input)):\n",
    "        word_encoding = copy.deepcopy(list_of_zeroes)\n",
    "        word_encoding[i] = 1\n",
    "        encoded_vocab.append(word_encoding)\n",
    "    return encoded_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the mapping words for input.This is for skipgram model.\n",
    "def generate_mapping(vocab_input,window_size):\n",
    "    mapped_words = {}\n",
    "    for index, word in enumerate(vocab_input):\n",
    "        neighbour_words = []\n",
    "        try:\n",
    "            for i in range(window_size):\n",
    "                if(index-(i+1) >= 0):\n",
    "                    neighbour_words.append(index-(i+1))\n",
    "                if(index + i +1 <= len(vocab_input) -1):\n",
    "                    neighbour_words.append(index+i+1)\n",
    "                neighbour_words.sort()\n",
    "                mapped_words[index] = neighbour_words\n",
    "        except:\n",
    "            print(\"index out of bounds\")\n",
    "    return mapped_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the one hot vectors into matrix.This can be taken as input to neural network.\n",
    "def onehotvec_to_matrix(list_of_lists):\n",
    "    mat_input = np.array(list_of_lists)\n",
    "    return mat_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "# function for random intialisation of weights of a matrix\n",
    "def initialize_weight_vec(x,y):\n",
    "    weight_vect = np.random.rand(x,y)\n",
    "    return weight_vect\n",
    "\n",
    "# function for multiplication of matrices\n",
    "def mat_multiply(x,y):\n",
    "    result_vec = np.zeros((len(x),len(y[0])))\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(y[0])):\n",
    "            for k in range(len(y)):\n",
    "                result_vec[i][j] += x[i][k]*y[k][j]\n",
    "    return result_vec\n",
    "\n",
    "# function for applying sigmoid function. output will be in [0-1] range\n",
    "def apply_sigmoid(x): \n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(x[0])):\n",
    "            x[i][j] = 1.0/(1.0 + exp(-x[i][j]))\n",
    "    return x\n",
    "\n",
    "# function for softmax. The sum of probabilities of all elements of row will be 1.\n",
    "def apply_softmax(x):\n",
    "    for i in range(len(x)):\n",
    "        new_list = x[i]\n",
    "        t = 0\n",
    "        for k in range(len(new_list)):\n",
    "            t += exp(new_list[k])\n",
    "        for j in range(len(x[0])):\n",
    "            x[i][j] = exp(x[i][j])/t\n",
    "    return x\n",
    "\n",
    "def forward_Propagation(input_to_hidden_wt_vec,hidden_to_output_wt_vec ):\n",
    "\n",
    "    # initialising w1 with random weights    \n",
    "    #input_to_hidden_wt_vec = initialize_weight_vec(len(input_mat),2)\n",
    "    #w1T*X\n",
    "    weight_transpose_times_input = np.matmul(input_to_hidden_wt_vec.T,input_mat)\n",
    "    #sigmoid(w1T*X) = x'\n",
    "    weights_after_activation = apply_sigmoid(weight_transpose_times_input)\n",
    "    # initialising w2 with random weights \n",
    "    #hidden_to_output_wt_vec = initialize_weight_vec(2,len(input_mat))\n",
    "    #print(hidden_to_output_wt_vec)\n",
    "    # w2T * X'\n",
    "    weight_transpose_times_hidden = np.matmul(hidden_to_output_wt_vec.T,weights_after_activation)\n",
    "    # softmax(w2T * X')\n",
    "    predicted_output = apply_softmax(weight_transpose_times_hidden)\n",
    "    \n",
    "    return weights_after_activation,predicted_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_error_mat(predicted_output,mapped_words,input_mat):\n",
    "    error_mat = np.zeros((len(predicted_output),len(predicted_output)))\n",
    "    for i in range(len(predicted_output)):\n",
    "        neighbours = mapped_words[i]\n",
    "        error = [0]*len(predicted_output[i])\n",
    "        for j in range(len(neighbours)):\n",
    "            error +=  (predicted_output[j] - input_mat[j])\n",
    "        window_size = len(neighbours)\n",
    "        error = [float(x)/window_size for x in error]\n",
    "        error_mat[i] = error\n",
    "    return error_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_derivative(x):\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(x[0])):\n",
    "            if i == j:\n",
    "                x[i][j] = np.multiply(x[i][j],(1-x[i][j]))\n",
    "            else:\n",
    "                x[i][j] = -(np.multiply(x[i][j],x[i][j]))\n",
    "    return x\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(x[0])):\n",
    "            x[i][j] = np.multiply(x[i][j],(1-x[i][j]))\n",
    "    return x\n",
    "\n",
    "def gen_column_error(x):\n",
    "    row_mat = np.zeros((len(x),1))\n",
    "    for i in range(len(x)):\n",
    "        row_mat[i] = sum(x[i][:])\n",
    "    return row_mat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(weight_matrix,delta_matrix,learning_rate,input_matrix):\n",
    "    delta_times_input = np.matmul(input_matrix,delta_matrix.T)\n",
    "    delta_times_input = learning_rate*delta_times_input\n",
    "    print(weight_matrix)\n",
    "    updated_weight_matrix = weight_matrix + delta_times_input\n",
    "    return updated_weight_matrix\n",
    "\n",
    "def bp_error_and_update_weights(weights_after_activation,predicted_output,input_to_hidden_wt_vec,hidden_to_output_wt_vec):\n",
    "    \n",
    "    ## generate error matrix at output layer\n",
    "    error_mat = gen_error_mat(predicted_output,mapped_words,input_mat)\n",
    "    #print(error_mat)\n",
    "   \n",
    "    ### compute delta matrix at output layer\n",
    "    derivative_output = softmax_derivative(predicted_output)\n",
    "    error_mat = gen_column_error(error_mat)\n",
    "    delta_mat_output = error_mat*derivative_output\n",
    "    #print(delta_mat_output)\n",
    "    \n",
    "    ## propagate delta values backwards to hidden layer and compute error\n",
    "    error_mat = delta_mat_output.dot(hidden_to_output_wt_vec.T)\n",
    "    derivative_hidden = sigmoid_derivative(weights_after_activation)\n",
    "    error_mat = gen_column_error(error_mat)\n",
    "    error_mat = error_mat.T\n",
    "    \n",
    "    ### calculate delta at hidden layer\n",
    "    delta_mat_hidden = error_mat*derivative_hidden\n",
    "    \n",
    "    \n",
    "    ### updating weights after completion of backpropagation\n",
    "    input_to_hidden_wt_vec = update_weights(input_to_hidden_wt_vec,delta_mat_hidden,0.5,input_mat)\n",
    "    hidden_to_output_wt_vec = update_weights(hidden_to_output_wt_vec,delta_mat_output,0.5,weights_after_activation)\n",
    "\n",
    "    return input_to_hidden_wt_vec,hidden_to_output_wt_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SkipGram_NN(input_mat,window_size,learning_rate):\n",
    "    epochs = 3\n",
    "    no_of_hidden_layers = 2\n",
    "    \n",
    "    # initialise weight matrices\n",
    "    input_to_hidden_wt_vec = initialize_weight_vec(len(input_mat),no_of_hidden_layers)\n",
    "    hidden_to_output_wt_vec = initialize_weight_vec(no_of_hidden_layers,len(input_mat))\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print(\"entered epoch \" + str(i))\n",
    "        weights_after_activation,predicted_output = forward_Propagation(input_to_hidden_wt_vec,hidden_to_output_wt_vec )\n",
    "        input_to_hidden_wt_vec,hidden_to_output_wt_vec = bp_error_and_update_weights(weights_after_activation,predicted_output,input_to_hidden_wt_vec,hidden_to_output_wt_vec)\n",
    "           \n",
    "    return hidden_to_output_wt_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered epoch 0\n",
      "[[ 0.86967189  0.2137422 ]\n",
      " [ 0.91891183  0.97953628]\n",
      " [ 0.3176267   0.3507927 ]\n",
      " [ 0.81186891  0.37057071]\n",
      " [ 0.59007253  0.43194808]\n",
      " [ 0.86417176  0.31559078]]\n",
      "[[ 0.64347001  0.67316424  0.33836549  0.97748249  0.82265267  0.92314928]\n",
      " [ 0.77726255  0.50629436  0.1658538   0.14600902  0.99424755  0.26639942]]\n",
      "entered epoch 1\n",
      "[[ 0.86967189  0.2137422 ]\n",
      " [ 0.91891183  0.97953628]\n",
      " [ 0.3176267   0.3507927 ]\n",
      " [ 0.81186891  0.37057071]\n",
      " [ 0.59007253  0.43194808]\n",
      " [ 0.86417176  0.31559078]]\n",
      "[[ 0.64347001  0.67316424  0.33836549  0.97748249  0.82265267  0.92314928]\n",
      " [ 0.77726255  0.50629436  0.1658538   0.14600902  0.99424755  0.26639942]]\n",
      "entered epoch 2\n",
      "[[ 0.86967189  0.2137422 ]\n",
      " [ 0.91891183  0.97953628]\n",
      " [ 0.3176267   0.3507927 ]\n",
      " [ 0.81186891  0.37057071]\n",
      " [ 0.59007253  0.43194808]\n",
      " [ 0.86417176  0.31559078]]\n",
      "[[ 0.64347001  0.67316424  0.33836549  0.97748249  0.82265267  0.92314928]\n",
      " [ 0.77726255  0.50629436  0.1658538   0.14600902  0.99424755  0.26639942]]\n",
      "[[ 0.64347001  0.77726255]\n",
      " [ 0.67316424  0.50629436]\n",
      " [ 0.33836549  0.1658538 ]\n",
      " [ 0.97748249  0.14600902]\n",
      " [ 0.82265267  0.99424755]\n",
      " [ 0.92314928  0.26639942]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "corpus = \"my name is subbareddy my mother name is venkayemma\"  ## input corpus\n",
    "vocab = process_corp(corpus) ## preprocess_corpus\n",
    "list_of_zeroes = [0]*len(vocab) ## generate list of size vocab\n",
    "encoded_vocab = one_hot_encoding(vocab,list_of_zeroes) ## one_hot_encoding of vocab\n",
    "window_size = 2    ## window_size \n",
    "learning_rate = 0.5\n",
    "mapped_words = generate_mapping(vocab,window_size)\n",
    "input_mat = onehotvec_to_matrix(encoded_vocab)\n",
    "\n",
    "final_weight_vector = SkipGram_NN(input_mat,window_size,learning_rate)\n",
    "final_weight_vector = final_weight_vector.T\n",
    "\n",
    "print(final_weight_vector)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"skipgram_final_weights.csv\", final_weight_vector, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.index(\"subba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.33836549  0.1658538 ]\n"
     ]
    }
   ],
   "source": [
    "word = 'subba'\n",
    "input_index = vocab.index(word)\n",
    "input_embedding = final_weight_vector[input_index][:]\n",
    "print(input_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "printing top 10 neighbours from final weight vec. Take word in strings.\n",
    "def Top_K(Input_word,vocab,weight_embeddings,K):\n",
    "    input_index = vocab.index(Input_word)\n",
    "    input_embedding = weight_embeddings[input_index][:]\n",
    "    return "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
